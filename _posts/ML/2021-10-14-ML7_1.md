---
layout: post
title: '기계학습'
subtitle: '7-1주차_선형회귀 모델'
date: 2021-10-14 18:30:00 +0900
categories: 'ML'
use_math: true
---

## 경사 하강법 실습

* $Y=4+3X+\epsilon$ ($0\le X \le 2$)를 따르는 샘플 100개를 생성한 후 선형회귀를 적용 

- SVD를 이용한 선형회귀인 LinearRegression, SGD를 이용한 선형회귀인 SGDRegressor, 배치 경사 하강법, 미니배치 경사 하강법을 비교 실습 

```python
# 데이터 샘플 생성 

import numpy as np 
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import SGDRegressor

import matplotlib.pyplot as plt 
%matplotlib inline

np.random.seed(20) #랜덤시드 고정

X = 2 * np.random.rand(100,1) #0~1의 균일분포 표준정규분포 난수를 matrix array(m,n) 생성
y = 4 + 3 * X + np.random.randn(100,1) #평균0, 표준편차1의 가우시안 표준정규분포 난수를 matrix array(m,n) 생성

plt.plot(X,y, 'b.') #파란색으로 산점도 그리기
plt.xlabel("$x_1$", fontsize=10)
plt.ylabel("$y$", rotation=0, fontsize=10)
plt.axis([0, 2, 0, 15]) #plt.axis([xmin,xmax,ymin,ymax]) 
```

```python
# 각 특성벡터의 첫 번째 좌표에 bias에 대응되는 1을 추가하여 Xb로 수정 
# 여러 가지 경사 하강법을 직접 구현해 볼 때 사용

Xb = np.column_stack((np.ones((100,1)), X))  ##배열을 열로 만들어서 붙이기

X_new = np.array([[0],[2]])
Xb_new = np.column_stack((np.ones((2,1)), X_new))
```



### LinearRegression 

```python
n_reg_svd = LinearRegression() 
lin_reg_svd.fit(X, y)
theta_svd = np.array([lin_reg_svd.intercept_, lin_reg_svd.coef_],dtype=float) #intercept_: 편향, coef_: 가중치
theta_svd 

#array([[3.8115615 ],
       [3.23892352]])
       
y_predict_svd = lin_reg_svd.predict(X_new)
y_predict_svd

#array([[ 3.8115615 ],
       [10.28940854]])
    
# 모델의 예측을 그래프에 나타내기 
plt.plot(X, y, 'b.') #파란색 점으로 산점도
plt.plot(X_new, y_predict_svd, 'r-') #빨간색 선으로 회귀직선 그리기
plt.xlabel("$x_1$", fontsize=10) #x좌표축
plt.ylabel("$y$", rotation=0, fontsize=10) #y좌표축
plt.axis([0, 2, 0, 15]) 
plt.title("Linear regression with SVD") #제목
```



### 배치 경사 하강법 1

- 그래디언트를 구하는 식 --- (2) 

$$
(\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})) = \dfrac 2 m \mathbf X^{\rm T}(\mathbf X\boldsymbol{\theta}-\mathbf y)=\mathbf 0
$$

* $\{\theta}$를 업데이트하는 식 --- (3)

$$
{\theta}^{(t+1)}:= \boldsymbol{\theta}^{(t)}-\alpha \nabla_{{\theta}}L({\theta}^{(t)})\ (t\ge 0)
$$

```python
eta = 0.1 # 학습률 
n_epochs = 1000 # epoch 수 
m = 100 # 샘플수 

theta_bgd = np.random.randn(2,1)  # 무작위로 theta 초깃값 설정 

for iteration in range(n_epochs):
    gradients = 2/m * Xb.T.dot(Xb.dot(theta_bgd) -y)   # 설명의 식 (2) 구현
    theta_bgd = theta_bgd - eta * gradients                # 설명의 식 (3) 구현 
```

