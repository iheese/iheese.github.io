---
layout: post
title: '기계학습_로지스틱 회귀와 소프트맥스 회귀 '
subtitle: '9주차_로지스틱 회귀, 시그모이드 함수, 소프트맥스 회귀'
date: 2021-10-26 12:00:00 +0900
categories: 'ML'
use_math: true
---

## 로지스틱 회귀(Logistic Regression) 의 학습 모델

- 레이블이 1, 0인 두 개의 클래스에 대한 분류문제에서 샘플이 특정 클래스에 속할 확률을 추정하는 지도학습의 한 가지

- 선형회귀 모델과 같이 입력 특성의 가중치의 합(편향 포함) $\theta^{\rm T}\mathbf x = \theta_0+\theta_1 x_1+\cdots +\theta_nx_n$을 계산한 다음 시그모이드 함수(sigmoid) $\sigma(t)=\dfrac{1}{1+\exp(-t)}$를 취한 값 $\sigma (\theta^{\rm T} \mathbf x)$를 ${\rm P}(Y=1 \mid X= \mathbf x)$ 에 대한 추정값 $\hat p(\mathbf x)$로 추정하는 모델. 

  

- 즉, 모델 파라미터 ${\theta}=(\theta_0,\cdots,\theta_n)^{\rm T}$에 대한 로지스틱 회귀 모델을 $h_{\theta}$라 할 때

  
  $$
  \hat p(\mathbf x) = h_{\theta}(\mathbf x)= \sigma({\theta}^{\rm T}\mathbf x)
  =\dfrac{1}{1+\exp(-{\theta}^{\rm T}\mathbf x)}=\dfrac{1}{1+\exp(-(\theta_0+\theta_1x_1+\cdots+\theta_n x_n))} \cdots(1)
  $$
  

- 시그모이드 함수를 생각하는 이유:

  > - 확률 $0<p<1$에 대해 오즈(odds) $\dfrac{p}{1-p}$는 $(0,\infty)$사이의 값을 가지므로 로그-오즈(log-odds) 또는 로짓이라 부르는 $\ln \left(\dfrac{p}{1-p}\right)$는 $(-\infty, \infty)$ 사이의 값을 가짐 
  > - 이 로짓을 선형모델로 추정하는 것이 로지스틱 회귀모델. 즉, $\ln (\dfrac{p}{1-p})=\theta^{\rm T}\mathbf x$으로부터 $p$를 구하면 식(1)이 나옴 

- 로지스틱 회귀 모델을 통한 레이블의 예측:

  > - 샘플 $\mathbf x$가 양성 클래스(y=1)에 속할 확률 $\hat p(\mathbf x)=h_\theta}(\mathbf x)$를 추정한 후 다음과 같이 예측 $\hat y$를 구함 
  >   $$
  >   \hat y = \begin{cases} 0 & \text{ if  }& \hat p(\mathbf x)<0.5\\ 1 &\text{ if }& \hat p(\mathbf x)\ge0.5
  >   \end{cases}
  >   $$

- 

### 시그모이드 함수 그려보기

```python
import numpy as np
import matplotlib.pyplot as plt 
%matplotlib inline

def sigmoid(x):
    return 1/(1+np.exp(-x))

xlist = np.linspace(-10,10,1000)
ylist = sigmoid(xlist)

plt.plot(xlist,ylist, label=r"$\sigma (t)=\dfrac{1}{1+\exp(-t)}$")

plt.plot([-10,10],[0.5,0.5], 'r--', linewidth=0.6)
plt.plot([-10,10],[1,1], 'r--', linewidth=0.6)
plt.plot([-10,10],[0,0], 'r--', linewidth=0.6)
plt.plot([0,0],[0,1],'r--', linewidth=0.6)

plt.legend(loc="upper left", fontsize=10)
```

![sigmoid](/img/posts/ML9/sigmoid.png)

<br>

## 로지스틱 회귀모델의 학습

- 훈련 데이터셋 $\{(\mathbf x_i,y_i) \mid1\le i \le m\}$이 주어질 때, 다음과 같이 정의되는 로지스틱 회귀의 비용함수(로그손실 함수) 
  $$
  J({\theta}) = -\dfrac 1 m \sum_{i=1}^m
  (y_i \ln p_i + (1-y_i)\ln (1-p_i)) \\
  (단, p_i= \sigma({\theta}^{\rm T}\mathbf x_i)) \cdots (2)
  $$
  

  

  가 최소가 되는 모델 파라미터 $\theta$를 구하는 것  

  > - $\text{P}(Y_i=1 \mid X=\mathbf x_i)=p_i$이므로 $\text{P}(Y_i=0 \mid X=\mathbf x_i)=1-p_i$이고 훈련 데이터셋은 iid 확률변수 $(X_i,Y_i)$ $(1\le i \le m)$에 대한 값이므로 로지스틱 회귀 모델에 대한 비용함수가 최소가 되는 $\theta$를 구하는 것은 $\theta$에 대한 로그우도함수 $\ell(\theta)= \ln(\text{P}(y_1,\cdots,y_m \mid \mathbf x_1,\cdots,\mathbf x_m:{\theta}))$가 최대가 되는 $\theta$를 구하는 것과 같다. 

  
  $$
  \begin{align}
  J(\theta)=&  -\dfrac 1 m \ln\left(\prod_{i=1}^m p_i^{y_i} (1-p_i)^{(1-y_i)}\right) \\
  =& -\dfrac 1 m \ln\left(\text{P}(Y_1=y_1,\cdots,Y_m=y_m|X_1=\mathbf x_1,\cdots,X_m=\mathbf x_m :\theta\right)
  \end{align}
  $$
  

  - 비용함수 $J(\theta)$는 $\theta$에 대해 아래로 볼록한 함수이므로 최솟값이 존재함을 보장할 수 있지만, 정규방정식처럼 해를 구하는 공식은 없음 

  > - 경사하강법 또는 다른 최적화 알고리즘을 이용하여 해의 근삿값을 구함 (`sklearn.linear_model` 모듈의 `LogisticRegression`의 `solver`참고)
  >
  > - 배치 경사하강법을 적용할 때 비용함수에 대한 그래디언트 벡터 $\boldsymbol{\nabla_{\theta}}J(\theta)$ : 각 $i$ ($1\le i\le m)$에 대해  $\nabla_\theta J(\theta)$의 $j$번째 성분은 
  >   $$
  >   \dfrac{\partial}{\partial \theta_j} J(\theta)= \dfrac 1 m \sum_{i=1}^m 
  >   (\sigma({\theta}^{\rm T}\mathbf x_i)-y_i)x_{ij},\\ \mathbf x_i=(1,x_{i1},\cdots,x_{in})
  >   $$

  - `sklearn.linear_model` 모듈의 `LogisticRegression` [API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression) 또는 확률적 경사하강법을 이용하는 `sklearn.linear_model` 모듈의 `SGDClassifier` [API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html?highlight=sgdclassifier#sklearn.linear_model.SGDClassifier)를 사용(이 경우 `loss="log"`로 설정해야 로지스틱 회귀)

    >- 다른 선형회귀 모델처럼 로지스틱 회귀 모델도 $\ell_1$, $\ell_2$ 규제항을 사용하여 규제할 수 있으며 `LogisticRegression`은 $\ell_2$ 페널티를 기본으로 함 (`penalty="l2"`, `penalty="none"`과 같이 사용)
    >
    >- 규제가 있는 `SGDClassifier(loss='log')` 모델의 경우 선형회귀 모델(`SGDRegressor`)의 경우처럼 규제를 조절하는 하이퍼파라미터가 `alpha`이지만, `LogisticRegression`의 경우에는 `alpha`의 역수에 해당하는 `C`가 규제를 조절하는 하이퍼파라미터 
    >
    >  > - C의 기본값은 1, C가 커지면 규제가 작아지고, C가 작아지면 규제가 커짐

  - 로지스틱 회귀 모델로 LogisticRegression을 사용하는 경우

    > - 최적화에 사용되는 알고리즘은 `solver`를 통해 선택할 수 있으며 기본값은 `solver="lbfgs"` (선택한 페널티 항에 대해 사용할 수 있는 최적화 알고리즘에 제한이 있으므로 자세한 내용은 위 API를 참고) 
    >
    >   > - 최적화: 최소, 최대를 구하는 것 , 최대를 구하는 것에 음수를 취하면 최소를 구하는 것과 같아짐
    >
    > - 이진 분류가 아닌 경우, 즉, 클래스의 종류가 $3$이상일 때는 입력변수 `multi_class`를 `multi_class="multinomial"`로 두면 뒤에 설명할 소프트맥스 회귀 모델이 됨 
    >
    >   > - `multi_class="ovr"`이면 일대다 전략을 사용 
    >
    > - 학습 후 예측은 `predict` 메소드를 이용하고, 확률값은 `predict_proba` 메소드를 이용 

    - 학습된 객체의 `score` 메소드는 정확도(accuracy)를 사용 

      <br>

### 로지스틱 회귀 실습

