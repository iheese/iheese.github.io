---
layout: post
title: '기계학습'
subtitle: '수업 3주차'
date: 2021-09-16 11:30:00 +0900
categories: 'ML'
use_math: true
comments: true
---

## 확률 기초 복습

>* 이산확률변수 $X$에 대한 확률질량함수(PMF: Probability Mass Function) $p(x)$:  
>   * $X$가 취할 수 있는 모든 $x_i\ (i=1,\cdots, n)$에 대해 $p(x_i)\ge 0$
>   * $\displaystyle{\sum_{i=1}^n p(x_i) = 1}$ (또는 $x\neq x_i$일 때 $p(x)=0$으로 정의하면 $\displaystyle{\sum_{x\in \mathbb R} p(x) = 1}$) 
>   * 복습할 내용  
>>* [이산확률변수 (기댓값, 분산)](https://terms.naver.com/entry.nhn?docId=3338094&ref=y&cid=47324&categoryId=47324)  
>>* [이산확률분포 (베르누이 분포, 이항분포, 다항분포)](https://terms.naver.com/entry.nhn?docId=3338096&ref=y&cid=47324&categoryId=47324)  
>>* [확률질량함수](https://terms.naver.com/entry.nhn?docId=3405419&ref=y&cid=47324&categoryId=47324) 

>* 연속확률변수 $X$에 대한 확률밀도함수(PDF: Probability Density Function) $p(x)$:  
>    * $X$가 취할 수 있는 모든 $a\le x \le b$에 대해 $p(x)\ge 0$  
>    * $\displaystyle{\int_a^b p(x)dx=1}$  (또는 $x\notin [a,b]$일 때 $p(x)=0$으로 정의하면 $\int_{\mathbb R} p(x)dx=1$)
>    * 복습할 내용  
>> * [연속확률변수 (기댓값, 분산)](https://terms.naver.com/entry.nhn?docId=3338172&ref=y&cid=47324&categoryId=47324)  
>> * [연속확률분포 (균등분포, 정규분포, 카이제곱분포)](https://terms.naver.com/entry.nhn?docId=4125375&ref=y&cid=60207&categoryId=60207)  
>> * [확률밀도함수](https://terms.naver.com/entry.nhn?docId=3405418&ref=y&cid=47324&categoryId=47324)  

> * 확률밀도함수와 확률질량함수를 통칭하여 확률함수 : 확률함수를 아는 것은 확률분포를 아는 것과 같음
>> * 복습할 내용  
>>      * [누적분포함수](https://terms.naver.com/entry.nhn?docId=3405007&ref=y&cid=47324&categoryId=47324)
> * 누적분포함수(CDF: Cumulative Distribution Function)  
>>    * 간단히 분포함수라고 함  
>>    * $X$가 이산확률변수일 때 누적분포함수 $F(x) = \displaystyle{\sum_{k\le x}p(k)}$  
>>    * $X$가 연속확률변수일 때 누적분포함수 $F(x) = \displaystyle{\int_{-\infty}^x p(x)dx}$
>>    * 확률 $P(a\le X \le b) = F(b)-F(a)$

>* 결합확률분포, 주변확률분포, 독립변수  
>    * 복습할 내용  
>>    * [결합확률질량함수, 주변확률질량함수](https://terms.naver.com/entry.nhn?docId=4125154&cid=60207&categoryId=60207)  
>>    * [결합확률밀도함수, 주변확률밀도함수](https://terms.naver.com/entry.nhn?docId=3404948&ref=y&cid=47324&categoryId=47324)  
>>    * [결합분포함수](https://terms.naver.com/entry.nhn?docId=3404946&ref=y&cid=47324&categoryId=47324)  
>>    * [독립변수](https://terms.naver.com/entry.nhn?docId=3338159&cid=47324&categoryId=47324)  
>>    * [공분산, 상관계수](https://terms.naver.com/entry.nhn?docId=3404964&cid=47324&categoryId=47324)  
>    * 두 확률변수 $X$와 $Y$가 독립:  
>>* 결합분포함수 $F_{X,Y}$와 각각의 분포함수(주변분포함수) $F_X$, $F_Y$에 대해 
$$F_{X,Y}(x,y)=F_X(x)F_Y(y)$$  
>>* 결합확률함수 $p_{X,Y}$와 각각의 확률함수 $p_X$, $p_Y$에 대해 $$p_{X,Y}(x,y)=p_X(x)p_Y(y)$$  
>    * 두 확률변수 $X$와 $Y$의 공분산 ${\rm Cov}(X,Y)$이 $0$일 때, 두 확률변수는 **상관없다(uncorrelated)**고 함  
>>* 두 확률변수 $X$와 $Y$가 독립이면 두 확률변수는 상관없다. 하지만 역은 성립하지 않음  
>>* (역이 성립하지 않는 예:  <span style="color:red">**check**</span>) 두 확률변수 $X,Y$의 결합 확률밀도함수가 $p(x,y)=\begin{cases} 1 &|x|+|y|\le \frac 1{\sqrt 2}\\ 0 & \text{그 외의 경우}\end{cases}$  
>    * 연속확률변수 $X$와 이산확률변수 $Y$의 결합확률함수가 $p(x,y)$일 때,  
>>* $X$에 대한 주변확률밀도함수 $p_X(x)= \displaystyle{\sum_{y\in\mathbb R}p(x,y)}$  
>>* $Y$에 대한 주변확률질량함수 $p_Y(y)= \displaystyle{\int_{-\infty}^{\infty}p(x,y)dx}$  


> * 조건부 확률, 조건부 분포함수, 조건부 확률함수  
>     * 조건부 확률함수 : 두 확률변수 $X,Y$에 대한 결합확률함수가 $p_{X,Y}$일 때  
>>  * $X=x$가 주어질 때 $Y$의 조건부 확률함수 $p_{Y|X}(y) = \dfrac{p_{X,Y}(x,y)}{p_X(x)}$  
>>  * $Y=y$가 주어질 때 $X$의 조건부 확률함수 $p_{X|Y}(x) = \dfrac{p_{X,Y}(x,y)}{p_Y(y)}$
>     * 복습할 내용  
>>    * [조건부 확률, 베이즈의 정리](https://terms.naver.com/entry.nhn?docId=3338502&cid=47324&categoryId=47324)  
>>    * [조건부 분포함수, 조건부 확률질량함수](https://terms.naver.com/entry.nhn?docId=3338190&ref=y&cid=47324&categoryId=47324)  
>>    * [조건부 분포함수, 조건부 확률밀도함수](https://terms.naver.com/entry.nhn?docId=3338189&ref=y&cid=47324&categoryId=47324)  
>
>  * 확률변수 $X$와 이산확률변수 $Y$의 결합확률함수가 $p=p_{X,Y}$일 때
>>* $p(x,y) = p_X(x)p_{Y|X}(y|x)=p_Y(y)p_{X|Y}(x|y)$  
>>* $p_X(x) = \displaystyle{\sum_{y\in \mathbb R}p(x,y)}$  
>>* 베이즈의 정리를 적용하면  $X=x$가 주어질 때, $Y$의 조건부 확률질량함수를 다음과 같이 쓸 수 있음  
>>$$p_{Y|X}(y|x)= \dfrac{p(x,y)}{p_X(x)}=\dfrac{p_Y(y)p_{X|Y}(x|y)}{p_X(x)}
=\dfrac{p_Y(y)p_{X|Y}(x|y)}{\sum_{y\in \mathbb R}p_Y(y)p_{X|Y}(x|y)}$$  
>>* 이때, $p_{Y|X}(y|x)$를 $Y$의 사후확률(posteriori probability)함수, $p_Y(y)$를 $Y$의 사전확률(priori probability)함수, $p_X(x)$를 근거(evidence)라고 함  

>* 연속확률변수 $X$가 $d$차원, 즉 $X=(X_1,X_2,\cdots,X_d)^T$일 때:  
>> * $X$의 기댓값 ${\rm E}(X) = ({\rm E}_{X_1}(X_1),\cdots,{\rm E}_{X_d}(X_d))^{\rm T}$  
>>     * $d=2$일 때, $$\begin{aligned} 
    {\rm E}(X) =& (\int_{\mathbb R}\int_{\mathbb R} x_1 p(x_1,x_2)dx_1dx_2,\int_{\mathbb R}\int_{\mathbb R} x_2 p(x_1,x_2)dx_1dx_2)^{\rm T}\\
    =&(\int_{\mathbb R} x_1 p_{X_1}(x_1)dx_1,\int_{\mathbb R} x_2 p_{X_2}(x_2)dx_2)^{\rm T}
    \end{aligned}$$
>> * $X$의 분산-공분산 행렬 ${\rm V}(X) = E_X\bigl((X-{\rm E}(X))(X-{\rm E}(X))^{\rm T}\bigr)$  
>>     * $d=2$일 때, $$\begin{aligned}
    {\rm V}(X) = & {\rm E}\left(\begin{pmatrix} X_1-{\rm E}_{X_1}\\ X_2-{\rm E}_{X_2}\end{pmatrix} 
    \begin{pmatrix} X_1-{\rm E}_{X_1}& X_2-{\rm E}_{X_2}\end{pmatrix}\right)\\
    =&\begin{pmatrix} {\rm E}\bigl((X_1-{\rm E}_{X_1})(X_1-{\rm E}_{X_1})\bigr) & {\rm E}\bigl((X_1-{\rm E}_{X_1})(X_2-{\rm E}_{X_2})\bigr)\\{\rm E} \bigl((X_2-{\rm E}_{X_2})(X_1-{\rm E}_{X_1})\bigr) &{\rm E} \bigl((X_1-{\rm E}_{X_1})(X_1-{\rm E}_{X_1})\bigr) \end{pmatrix}\\
    =&\begin{pmatrix} {\rm V}(X_1) & {\rm Cov}(X_1,X_2)\\ {\rm Cov}(X_2,X_1) & {\rm V}(X_2)\end{pmatrix}
    \end{aligned}
    $$  
>>* ${\rm Cov}(X_1,X_1) = {\rm V}(X_1)$  

>* $d\times d$ 행렬 $A$와 확률변수 $X = (X_1,\cdots,X_d)^{\rm T}$에 대해  
>> * $\text{E}(AX) = A\text{E}(X)$  
>> * $\text{V}(AX) = A\text{V}(X)A^{\rm T}$



>* 다변수 정규분포 
>>* 복습할 내용 :  
>>>- 선형대수학: 행렬연산, 대칭행렬, 대칭행렬의 직교대각화, 이차형식 (해설이 있는 선형대수학(2판) 또는 다른 선형대수학 책 참고)
>>>- [다변수 정규분포](https://terms.naver.com/entry.nhn?docId=5669006&cid=60207&categoryId=60207)
>>* 평균벡터 $\mathbf \mu$, 공분산 행렬 ${\bf \Sigma}$인 다변수(다변량) 정규분포의 확률밀도 함수: $\mathbf x=(x_1,\cdots,x_d)^{\rm t}$라 할 때 
$$p(\mathbf x)=\dfrac 1{(\sqrt 2\pi)^d ({\rm det}{\mathbf \Sigma})^{\frac 1 2}} {\rm exp}\bigl(-\dfrac 1 2 (\mathbf x-\mathbf \mu)^{\rm T} \mathbf \Sigma ^{-1} (\mathbf x-\mathbf \mu)\bigr)$$  
>>* 공분산 행렬은 양의 정부호행렬(positive definite matrix): $\mathbf \Sigma$의 모든 고윳값이 양수  
>>* 특히, 공분산 행렬이 대각행렬 $\mathbf \Sigma = {\rm diag}(\sigma_1^2,\cdots,\sigma_d^2)$이고 $\mathbf \mu = (\mu_1,\cdots, \mu_d)^{\rm T}$라 하면 
$$p(\mathbf x)=p(x_1,\cdots,x_n) = \prod_{i=1}^d \dfrac 1{\sqrt{2\pi}\sigma_i}{\rm exp}\left(-\dfrac{(x_i-\mu_1)^2}{2\sigma^2}\right)$$

<img src=".\images\week3\multi_normal.png" width="900">

## 통계와 관련된 파이썬 패키지: scipy.stats 

* 이산확률분포의 예 : 이항분포 
>* $X\sim B(n,p)$
>* `scipy.stats`의 `binom`모듈: [사용법](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html?highlight=binom#scipy.stats.binom)


```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```


```python
from scipy.stats import binom
```


```python
# 이항분포 B(n,p)파라미터 설정 
p = 0.3
n = 100

# 이항분포 정의 
dist1 = binom(n,p)
```


```python
# 이항분포 확률질량함수 dist1.pmf 

Xlist = range(10,50)
Ylist = dist1.pmf(Xlist)
plt.plot(Xlist, Ylist)
```


```python
# 이항분포의 CDF: dist1.cdf

for n in range(10,100,10):
    print(f'{n}번 이하 성공할 확률:{dist1.cdf(n)*100:0.3f}')
```


```python
# m:mean (평균), v:variance(분산), s:skewness(왜도), k:kurtosis(첨도)
m,v,s,k = dist1.stats(moments='mvsk')

print(f'X가 B(100,0.3)를 따를 때, 평균:{m}, 분산:{v}, 왜도:{s}, 첨도:{k}')
```


```python
np.sqrt(v)*3
```

* 연속확률분포의 예: 정규분포  
>* $X\sim N(\mu,\sigma^2)$
>* `scipy.stats`의 `norm` 모듈 : [사용법](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html?highlight=norm) 


```python
from scipy.stats import norm
```


```python
mu = 50 
sigma = 5
dist2 = norm(mu,sigma)
```


```python
# 정규분포의 확률밀도함수 

Xlist = range(20,80)
Ylist = dist2.pdf(Xlist)
plt.plot(Xlist, Ylist)
```


```python
dist2.pdf(50)
```


```python
# 정규분포의 CDF 

print(f'P(40<=X<=60)={dist2.cdf(60)-dist2.cdf(40):5.2f}')
```


```python
# m:mean (평균), v:variance(분산), s:skewness(왜도), k:kurtosis(첨도)
m,v,s,k = dist2.stats(moments='mvsk')

print(f'X가 N(50,25)를 따를 때, 평균:{m}, 분산:{v}, 왜도:{s}, 첨도:{k}')
```


```python
# 여기서 그림을 그리는 부분은 필수적으로 알아야 하는 내용은 아닙니다. 
#(물론, 각 함수들을 찾아서 이해하고 활용할 수 있으면 도움이 됩니다.)
from scipy.stats import multivariate_normal

x, y = np.mgrid[-1:1:0.01, -1:1:.01]
pos = np.dstack((x, y))

rv1 = multivariate_normal(mean=[0, 0], cov=[[0.1, 0],[0, 0.1]])
rv2 = multivariate_normal(mean=[0, 0], cov=[[1, 0],[0, 1]])

rv3 = multivariate_normal(mean=[0, 0], cov=[[2, 0],[0, 1]])
rv4 = multivariate_normal(mean=[0, 0], cov=[[1, 0],[0, 2]])

rv5 = multivariate_normal(mean=[0, 0], cov=[[1, 0.5],[0.5, 1]])
rv6 = multivariate_normal(mean=[0, 0], cov=[[1, -0.5],[-0.5, 1]])

fig, subplots = plt.subplots(2, 3)
fig.set_figwidth(12)
fig.set_figheight(8)
subplots = subplots.reshape(-1)

subplots[0].contourf(x, y, rv1.pdf(pos), cmap='magma')
subplots[1].contourf(x, y, rv2.pdf(pos), cmap='magma')
subplots[2].contourf(x, y, rv3.pdf(pos), cmap='magma')
subplots[3].contourf(x, y, rv4.pdf(pos), cmap='magma')
subplots[4].contourf(x, y, rv5.pdf(pos), cmap='magma')
subplots[5].contourf(x, y, rv6.pdf(pos), cmap='magma')

subplots[0].set_title('mean=0 cov=[[0.1, 0],[0, 0.1]]')
subplots[1].set_title('mean=0 cov=[[1, 0],[0, 1]]')
subplots[2].set_title('mean=0 cov=[[2, 0],[0, 1]]')
subplots[3].set_title('mean=0 cov=[[1, 0],[0, 2]]')
subplots[4].set_title('mean=0 cov=[[1, 0.5],[0.5, 1]]')
subplots[5].set_title('mean=0 cov=[[1, -0.5],[-0.5, 1]]')
```


```python
rv1.pdf([1,2])
```

## 지도학습(분류문제) 모델의 종류 : 

### 판별모델(discriminative model) 과 생성모델(generative model)

* 각 샘플의 $d$차원 특성벡터 $\mathbf x$와 레이블 $y$ ($y\in \{y_1,\cdots,y_k\})$이 각각 미지의 확률분포를 따를 때, 
>* 확률변수 $X=(X_1,\cdots,X_d)$와 $Y$의 결합확률함수를 $p(X,Y)$라 하자. 
>* $X=\mathbf x$에 대한 레이블이 $Y=y_i$ $(1\le i \le k)$가 될 확률 $p_Y(y_i|X=\mathbf x)$
$$p_Y(y_i|X=\mathbf x)=\dfrac{p(\mathbf x,y_i)}{p_X(\mathbf x)}=\frac{p_Y(y_i)p_{X|Y}(\mathbf x|y_i)}{p_X(\mathbf x)}$$
>* $X=\mathbf x$가 주어질 때, 대응되는 레이블의 예측을 $\hat y={\rm argmax}\, p(y_i|\mathbf x)$로 구할 수 있음, 즉 판별함수 $\hat f(\mathbf x)={\rm argmax}\, p(y_i|\mathbf x)$
>* $p_Y(y)$와 각 클래스별 $X$의 분포 $p_{X|Y}(\mathbf x|y)$를 구하면, 이로부터 $p(\mathbf x,y)$를 구한 다음 $p(y|\mathbf x)$를 구할 수 있다. 이 경우 앞서 설명한 방식으로 판별함수 $\hat f$를 구할 수 있으므로 확률분포 $p(\mathbf x,y)$를 생성하는 것을 통해 판별함수를 구하는  학습모델을 **생성모델**이라 한다. 
>* 생성모델과 달리 $p(\mathbf x,y)$를 구하지 않고 $\hat f(\mathbf x)$만을 구성하는 학습모델을 **판별모델**이라고 한다. 
>* 생성모델과 판별모델의 장단점 

#### 다음 시간에 판별모델과 생성모델의 구체적인 예를 통해 기계학습의 전체 그림을 이해 
> * 다음 시간에 공부할 판별모델의 예 : 퍼셉트론 모델 ([scikit-learn 참고](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html?highlight=perceptron#sklearn.linear_model.Perceptron))
> * 다음 시간에 공부할 생성모델의 예 : 나이브베이즈 모델 ([scikit-learn 참고](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB))

## 연습문제 

1) 두 확률변수 $X,\ Y$의 공분산이 $\text{Cov}(X,Y)=0$이지만 두 확률변수가 독립이 아닌 예(강의 중  <span style="color:red">**check**</span> 표시)를 증명하시오.

2) $d\times d$ 행렬 $A$와 확률변수 $X = (X_1,\cdots,X_d)^{\rm T}$에 대해 다음이 성립함을 보이시오. 
> 2-1) $\text{E}(AX) = A\text{E}(X)$  
> 2-2) $\text{V}(AX) = A\text{V}(X)A^{\rm T}$



```python

```
