I"4<h1 id="앙상블-모델">앙상블 모델</h1>

<ul>
  <li>
    <p>앙상블: 조화 또는 통일을 의미</p>
  </li>
  <li>
    <p>어떤 데이터의 값을 예측할 때 여러 개의 모델을 조화롭게 학습시켜 그 모델의 예측 결과치를 이용하는 모델</p>
  </li>
  <li>
    <p>여러 개의 결정 트리를 결합하여 하나의 결정 트리보다 더 좋은 성능을 내는 머신 러닝 기법</p>
  </li>
  <li>
    <p>여러 개의 약 분류기를 결합하여 강 분류기를 만드는 것</p>
  </li>
  <li>
    <p>앙상블 모델의 종류</p>

    <blockquote>
      <ul>
        <li>
          <p>배깅(Bagging; Bootstrap Aggregation):</p>

          <ul>
            <li>
              <p>샘플을 여러 개 뽑아 각 모델을 학습 시켜 결과물을 집계하는 방법</p>
            </li>
            <li>
              <p>예를 들어 6개의 독립적인 결정 트리 모델이 있다. 4개는 A로 예측, 2개는 B로 예측했다면 A를 최종 결과로 예측하는 방식(분류 문제)</p>

              <blockquote>
                <ul>
                  <li>대표적인 예가 랜덤 포레스트</li>
                </ul>
              </blockquote>
            </li>
          </ul>
        </li>
        <li>
          <p>부스팅(Boosting):</p>

          <ul>
            <li>
              <p>가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법</p>
            </li>
            <li>
              <p>처음 모델이 예측하면 그 예측 결과에 따라 데이터에 가중치를 부여하고 다음 모델에 영향을 주는 방식</p>
            </li>
            <li>
              <p>잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만듬</p>
            </li>
            <li>
              <p>오답에 대해 높은 가중치, 정답에 대해 낮은 가중치를 부여 한 번 학습이 모두 끝나면 최종 모델 결과 예측에 영향을 준다.</p>
            </li>
          </ul>
        </li>
        <li>
          <p>부스팅은 배깅에 비해 에러가 적음</p>
        </li>
        <li>
          <p>하지만 부스팅은 속도가 느리고 overfitting 가능성이 있다.</p>
        </li>
      </ul>
    </blockquote>

    <p><img src="/img/posts/ML15/Baggingboosting.png" alt="bagging, boasting" width="300" /></p>

    <p>                                                          출처: swallow.github.io</p>
  </li>
</ul>

<p><br /></p>

<h2 id="랜덤포레스트random-forest">랜덤포레스트(Random Forest)</h2>

<ul>
  <li>sklearn.emsemble 모듈</li>
</ul>

<blockquote>
  <ul>
    <li>
      <p><code class="language-plaintext highlighter-rouge">RandomForestClassifier</code> 클래스(<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier">API</a>)</p>
    </li>
    <li>
      <p><code class="language-plaintext highlighter-rouge">RandomForestRegressor</code> 클래스 (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=randomforestregressor#sklearn.ensemble.RandomForestRegressor">API</a>)</p>
    </li>
  </ul>
</blockquote>

<ul>
  <li>
    <p>기본적인 아이디어</p>

    <blockquote>
      <ul>
        <li>
          <p>성능이 좋지 않은 예측기를 여러 개 만들고</p>

          <blockquote>
            <ul>
              <li>
                <p>분류의 경우에는 각 예측기에서 가장 많이 예측된 값으로 예측</p>
              </li>
              <li>
                <p>회귀의 경우에는 각 예측기의 평균으로 예측</p>
              </li>
            </ul>
          </blockquote>
        </li>
      </ul>
    </blockquote>
  </li>
</ul>

<h3 id="랜덤-포레스트-기본-구현">랜덤 포레스트 기본 구현</h3>

<ol>
  <li>
    <p><code class="language-plaintext highlighter-rouge">sklearn.model_selection</code> 모듈의 <code class="language-plaintext highlighter-rouge">ShuffleSplit</code>(<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html?highlight=shufflesplit#sklearn.model_selection.ShuffleSplit">API</a>)를 이용하여, 무작위로 선택된 100개의 샘플을 각각 갖는 훈련 세트의 서브셋을 1,000개 생성</p>
  </li>
  <li>
    <p>격자탐색을 통해 찾은 하이퍼파라미터를 이용하여 생성한 결정트리 예측기(4.1에서 구한 <code class="language-plaintext highlighter-rouge">best_clf</code>이용)를 훈련 세트의 각 서브셋으로 학습시켜 1000개의 약한 분류기를 생성</p>
  </li>
  <li>
    <p>테스트 세트 샘플에 대해 1,000개의 결정 트리 예측을 만들고 다수로 나온 예측을 반환하는 예측기 생성 (<code class="language-plaintext highlighter-rouge">scipy.stats</code> 모듈의 <code class="language-plaintext highlighter-rouge">mode()</code> 함수 이용)</p>
  </li>
  <li>
    <p>랜덤포레스트 (약한 분류기에 대한 다수 결정) 예측에 대한 정확도 비교</p>

    <p><br /></p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 랜덤포레스트 기본 구현 (1)
</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>

<span class="n">n_trees</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">n_instances</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">mini_sets</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">result_split</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_trees</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">-</span> <span class="n">n_instances</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="k">for</span> <span class="n">mini_train_index</span><span class="p">,</span> <span class="n">mini_test_index</span> <span class="ow">in</span> <span class="n">result_split</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_train</span><span class="p">):</span>
 <span class="n">X_mini_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">mini_train_index</span><span class="p">]</span>
 <span class="n">y_mini_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">mini_train_index</span><span class="p">]</span>
 <span class="n">mini_sets</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">X_mini_train</span><span class="p">,</span> <span class="n">y_mini_train</span><span class="p">))</span>
<span class="c1">#무작위로 분할하여 1000개 서브셋 생성
</span>
<span class="c1"># 랜덤포레스트 기본 구현 (2)
</span>
<span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>

<span class="n">forest</span> <span class="o">=</span> <span class="p">[</span><span class="n">clone</span><span class="p">(</span><span class="n">best_clf</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trees</span><span class="p">)]</span>

<span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">tree</span><span class="p">,</span> <span class="p">(</span><span class="n">X_mini_train</span><span class="p">,</span> <span class="n">y_mini_train</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">forest</span><span class="p">,</span> <span class="n">mini_sets</span><span class="p">):</span>
 <span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_mini_train</span><span class="p">,</span> <span class="n">y_mini_train</span><span class="p">)</span>


<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"1000개의 약한 학습기에 대한 정확도 평균:</span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">accuracy_scores</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

<span class="c1">#1000개의 약한 학습기에 대한 정확도 평균:0.856837
</span></code></pre></div></div>

<h1 id="랜덤포레스트-기본-구현-3">랜덤포레스트 기본 구현 (3)</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">empty</span><span class="p">([</span><span class="n">n_trees</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span>

<span class="k">for</span> <span class="n">tree_index</span><span class="p">,</span> <span class="n">tree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">forest</span><span class="p">):</span>
 <span class="n">Y_pred</span><span class="p">[</span><span class="n">tree_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">mode</span>

<span class="n">y_pred_votes</span><span class="p">,</span> <span class="n">n_votes</span> <span class="o">=</span> <span class="n">mode</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="c1"># 랜덤포레스트 기본 구현 (4)
</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"격자탐색 결정트리의 테스트 데이터셋 예측 정확도: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred3</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"랜덤포레스트의 테스트 데이터셋 예측 정확도: </span><span class="si">{</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_votes</span><span class="p">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span><span class="si">}</span><span class="s">"</span><span class="n">격자탐색</span> <span class="n">결정트리의</span> <span class="n">테스트</span> <span class="n">데이터셋</span> <span class="n">예측</span> <span class="n">정확도</span><span class="p">:</span> <span class="mf">0.9156666666666666</span><span class="p">)</span>

<span class="c1">#격자탐색 결정트리의 테스트 데이터셋 예측 정확도: 0.9156666666666666
#랜덤포레스트의 테스트 데이터셋 예측 정확도: 0.9166666666666666
</span></code></pre></div></div>

<p>Reference:</p>

<ul>
  <li>
    <p>기계학습_하길찬 교수님 수업을 듣고 공부한 내용입니다.</p>
  </li>
  <li>
    <p><a href="[머신러닝 - 11. 앙상블 학습 (Ensemble Learning): 배깅(Bagging)과 부스팅(Boosting)](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning-%EB%B0%B0%EA%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting)">앙상블 학습_귀퉁이 서재</a></p>
  </li>
</ul>
:ET